{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoorayoung/CSE-210-class-demo/blob/main/Copy_of_gpt2_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtiXE04uGB_U"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c052c94-3a2e-4556-d78c-ddc751ad781d"
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv4r-dKnSRKz"
      },
      "source": [
        "## I. Parse Text Sources\n",
        "First we'll load our text sources and create our vocabulary lists and encoders. \n",
        "\n",
        "There are ways we could do this in pure python, but using the tensorflow data structures and libraries allow us to keep things super-optimized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8RbnIjwHGoR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1418474-f6e8-4694-ef21-5ac7a2f02854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.gutenberg.org/files/1661/old/1661-8.txt\n",
            "598016/594960 [==============================] - 0s 0us/step\n",
            "606208/594960 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load file data\n",
        "path_to_file = tf.keras.utils.get_file('doyle.txt', 'https://www.gutenberg.org/files/1661/old/1661-8.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuU1pkRs8JyD",
        "outputId": "daea3526-de26-46d2-a1df-ecd6418b716a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 400Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 2.62Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 584Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:27, 18.0Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 739Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 4.54Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 4.31Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=path_to_file,\n",
        "              model_name='124M',\n",
        "              steps=1500,\n",
        "              restore_from='latest',\n",
        "              run_name='run3',\n",
        "              print_every=10,\n",
        "              sample_every=500\n",
        "              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vnpapaX8WKT",
        "outputId": "a7a81fd2-0024-4c94-a6a7-ae2136897578"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 150539 tokens\n",
            "Training...\n",
            "[10 | 27.77] loss=3.09 avg=3.09\n",
            "[20 | 49.34] loss=2.95 avg=3.02\n",
            "[30 | 71.31] loss=2.80 avg=2.95\n",
            "[40 | 93.77] loss=2.84 avg=2.92\n",
            "[50 | 116.75] loss=2.45 avg=2.82\n",
            "[60 | 140.22] loss=2.72 avg=2.80\n",
            "[70 | 163.49] loss=2.32 avg=2.73\n",
            "[80 | 186.54] loss=2.35 avg=2.68\n",
            "[90 | 209.67] loss=2.58 avg=2.67\n",
            "[100 | 232.93] loss=2.48 avg=2.65\n",
            "[110 | 256.16] loss=2.39 avg=2.63\n",
            "[120 | 279.36] loss=2.42 avg=2.61\n",
            "[130 | 302.55] loss=2.45 avg=2.60\n",
            "[140 | 325.74] loss=2.01 avg=2.55\n",
            "[150 | 348.94] loss=2.13 avg=2.52\n",
            "[160 | 372.12] loss=1.94 avg=2.48\n",
            "[170 | 395.33] loss=2.12 avg=2.46\n",
            "[180 | 418.57] loss=2.00 avg=2.43\n",
            "[190 | 441.79] loss=1.46 avg=2.37\n",
            "[200 | 465.01] loss=1.82 avg=2.34\n",
            "[210 | 488.23] loss=1.48 avg=2.30\n",
            "[220 | 511.44] loss=1.37 avg=2.25\n",
            "[230 | 534.65] loss=1.34 avg=2.21\n",
            "[240 | 557.84] loss=1.44 avg=2.17\n",
            "[250 | 581.00] loss=0.97 avg=2.12\n",
            "[260 | 604.16] loss=1.06 avg=2.07\n",
            "[270 | 627.32] loss=1.21 avg=2.04\n",
            "[280 | 650.47] loss=0.77 avg=1.98\n",
            "[290 | 673.62] loss=1.03 avg=1.95\n",
            "[300 | 696.80] loss=0.91 avg=1.91\n",
            "[310 | 719.97] loss=0.51 avg=1.85\n",
            "[320 | 743.16] loss=0.58 avg=1.81\n",
            "[330 | 766.34] loss=0.63 avg=1.77\n",
            "[340 | 789.55] loss=0.38 avg=1.72\n",
            "[350 | 812.75] loss=1.09 avg=1.70\n",
            "[360 | 835.97] loss=0.63 avg=1.66\n",
            "[370 | 859.18] loss=0.33 avg=1.62\n",
            "[380 | 882.37] loss=0.39 avg=1.58\n",
            "[390 | 905.58] loss=0.37 avg=1.54\n",
            "[400 | 928.78] loss=0.30 avg=1.51\n",
            "[410 | 951.95] loss=0.26 avg=1.47\n",
            "[420 | 975.11] loss=0.23 avg=1.43\n",
            "[430 | 998.28] loss=0.46 avg=1.40\n",
            "[440 | 1021.46] loss=0.21 avg=1.37\n",
            "[450 | 1044.65] loss=0.20 avg=1.34\n",
            "[460 | 1067.87] loss=0.25 avg=1.31\n",
            "[470 | 1091.07] loss=0.22 avg=1.28\n",
            "[480 | 1114.28] loss=0.17 avg=1.25\n",
            "[490 | 1137.48] loss=0.22 avg=1.22\n",
            "[500 | 1160.66] loss=0.12 avg=1.20\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "There was one more point on the panel to which I had\n",
            "dedications, and it was this:\n",
            "\n",
            "\"DEAR MR. HOLMES:--You have been of very important and vital importance to\n",
            "me since the very first message I received. I have frequently told you that\n",
            "I have seen Mr. Victor A. Morrell, the gentleman with the great\n",
            "coat, and that I had the great complimented for the\n",
            "success of your discovery. I have been most interested in it all\n",
            "and would not have missed your chance. Your remarks are in\n",
            "service to this work.\"\n",
            "\n",
            "\"I cannot thank you enough. I wish I knew all the steps by which\n",
            "I could trace down the man who had so kindly given me a ring. I\n",
            "owe no capital, but rather take pride in the hundred-yard\n",
            "right. Let me just run through some of the finer points.\" He put on\n",
            "his enormous white mask and bright shining wig, and returned to his natural\n",
            "colour.\n",
            "\n",
            "\"LET'S GO TO THE DUTTONS!\"\n",
            "\n",
            "\"We are far from finished yet. The reward offered by the\n",
            "agency is insufficient to enable us to set foot in the\n",
            "room. We must consider the practical and the military, as well as\n",
            "the commercial, means.\"\n",
            "\n",
            "\"We may start upon the political,\" said Colonel Openshaw. \"The inquiry will\n",
            "begin, first, in the nature of our enterprise, and second\n",
            "in the nature of the money which we shall acquire by our first\n",
            "test. We are going out in a blaze-mobbed band, and the only\n",
            "suspicion that this marks a break with the past is the fact that\n",
            "we have three captured men--three captured in an engagement, one\n",
            "in France, the other in Holland, and one in China. Let us\n",
            "stand in the back of the brougham and look as follows:\n",
            "\n",
            "\"1. THE BLUE BREAD.\n",
            "\n",
            "We have here a large supply of tobacco, and we have one or two\n",
            "of our own. Two of our own lay lying within the horsey\n",
            "baggy pockets of your harness. Three lives in a brougham, and the\n",
            "third is in a different case in a different box. What is the\n",
            "weighty subject, Colonel?\"\n",
            "\n",
            "\"My dear fellowmen, if you will keep your words to the effect\n",
            "that a man should keep his head above the ground when he\n",
            "is in a fight, it is certainly not in his nature. I will tell you\n",
            "what: I shall wear the headband of that fellow to the bed,\n",
            "Watson. He has pulled it up upon me in the muster of his\n",
            "cynical warfare, and upon my nerves it has worked in their favour\n",
            "a good deal. If I were in your position, I would have\n",
            "abled for you, as it were, to warn you that our\n",
            "boy was dangerously hot upon the scent of some deadly poison. A\n",
            "general alert was called yesterday in the province, and all was\n",
            "going on in the Surrey End like the previous night. To\n",
            "twice a week the Surrey End, and every Wednesday since.\n",
            "A man might have his life in him, but a chill is the loathing of a\n",
            "man\n",
            "and a spark is a deadly weapon. If he is not, it is all in his head\n",
            "and wants to be rid of. And yet none of this has ever\n",
            "come as a shock to me. I have been out to the End of the\n",
            "Bolton chain, and no one had been hurt in the last twenty minutes.\n",
            "The drive was less than pleasant, and there was a few minutes\n",
            "when I became unwell. My companion, perhaps in ignorance, I\n",
            "offered to be with you, but he on rare days came alone,\n",
            "and generally from the neighbourhood. I take it that he has taken no notice\n",
            "of anyone since he last drove.\n",
            "\n",
            "\"It is a most unpleasant thing for you to do, for I have experience of\n",
            "it. However, I shall not allow it to affect me in any way. I shall\n",
            "handpick my witnesses, and with these you may go on looking\n",
            "for that whom the law may give you their powers. I may even suggest\n",
            "your case, as it were, before the gaolman, and I may give you\n",
            "your own account of how the case came to be through your witness\n",
            "or through your informant. Whatever your intentions are,\n",
            "they may be of some assistance to you in your quest.\"\n",
            "\n",
            "\"I can hardly say that,\" I answered. \"I have my own set of\n",
            "plots, and I have nothing more to add. But you can shift it\n",
            "alongs as you please, and I can work it out with you as best you\n",
            "could. This is my own theory.\"\n",
            "\n",
            "\"Well, now!\" he usurred as he folded up the papers.\n",
            "\n",
            "[510 | 1197.03] loss=0.21 avg=1.17\n",
            "[520 | 1220.29] loss=0.16 avg=1.15\n",
            "[530 | 1243.53] loss=0.17 avg=1.12\n",
            "[540 | 1266.74] loss=0.12 avg=1.10\n",
            "[550 | 1289.94] loss=0.12 avg=1.08\n",
            "[560 | 1313.12] loss=0.10 avg=1.05\n",
            "[570 | 1336.29] loss=0.11 avg=1.03\n",
            "[580 | 1359.48] loss=0.09 avg=1.01\n",
            "[590 | 1382.68] loss=0.15 avg=0.99\n",
            "[600 | 1405.89] loss=0.11 avg=0.97\n",
            "[610 | 1429.10] loss=0.12 avg=0.95\n",
            "[620 | 1452.29] loss=0.08 avg=0.93\n",
            "[630 | 1475.52] loss=0.10 avg=0.92\n",
            "[640 | 1498.74] loss=0.09 avg=0.90\n",
            "[650 | 1521.98] loss=0.11 avg=0.88\n",
            "[660 | 1545.22] loss=0.08 avg=0.87\n",
            "[670 | 1568.43] loss=0.09 avg=0.85\n",
            "[680 | 1591.64] loss=0.08 avg=0.84\n",
            "[690 | 1614.81] loss=0.07 avg=0.82\n",
            "[700 | 1637.99] loss=0.09 avg=0.81\n",
            "[710 | 1661.17] loss=0.09 avg=0.79\n",
            "[720 | 1684.35] loss=0.07 avg=0.78\n",
            "[730 | 1707.54] loss=0.07 avg=0.76\n",
            "[740 | 1730.73] loss=0.09 avg=0.75\n",
            "[750 | 1753.94] loss=0.07 avg=0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=\"124M\",\n",
        "              prefix=\"The world seemed like such a peaceful place until the magic tree was discovered in London\",\n",
        "              length=100,\n",
        "              temperature=0.75,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "metadata": {
        "id": "dXqqy8vrqbIY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "2d2a0a21-3672-49c6-f946-499445f05026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fc48fa114b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m gpt2.generate(sess,\n\u001b[0m\u001b[1;32m      2\u001b[0m               \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"124M\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The world seemed like such a peaceful place until the magic tree was discovered in London\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gpt2' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of gpt2 working.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}